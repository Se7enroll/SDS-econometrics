{
<<<<<<< HEAD
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note:** In most sessions you will be solving exercises posed in a Jupyter notebook that looks like this one. Because you are cloning a Github repository that only we can push to, you should **NEVER EDIT** any of the files you pull from Github. Instead, what you should do, is either make a new notebook and write your solutions in there, or **make a copy of this notebook and save it somewhere else** on your computer, not inside the `sds` folder that you cloned, so you can write your answers in there. If you edit the notebook you pulled from Github, those edits (possible your solutions to the exercises) may be overwritten and lost the next time you pull from Github. This is important, so don't hesitate to ask if it is unclear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Set 1: Lending club\n",
    "\n",
    "*February 5, 2019*\n",
    "\n",
    "In this Exercise Set 1 we will investigate loan data from `Lending Club`. We will see how nested cross validation can be used  to compute a distribution of performance metrics for comparing different models. We will also try out bagging.\n",
    "\n",
    "Below we provide some code that helps by downloading and cleaning the data. Note this takes around 1 min if you have fast internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T20:03:29.726791Z",
     "start_time": "2020-02-13T20:03:29.516282Z"
    }
   },
   "outputs": [],
   "source": [
    "# Note: there are three .zip files with letter = a,b,c. \n",
    "# To ensure the files download in reasonable time we \n",
    "# only work with the first of the three. If you have time\n",
    "# you can modify this cell to download all three. \n",
    "\n",
    "import os\n",
    "import requests\n",
    "\n",
    "filenames = []\n",
    "base_url = 'https://resources.lendingclub.com/'\n",
    "\n",
    "letter = 'a'\n",
    "filename = f'LoanStats3{letter}.csv.zip'\n",
    "url = base_url+filename\n",
    "\n",
    "if not os.path.exists(filename):\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open(filename, 'wb').write(r.content)\n",
    "filenames.append(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target we are going to predict is whether loan is repaid. This can be extracted from the `loan_status` below. Features names that we work with:\n",
    "\n",
    "- **annual_inc**: The self-reported annual income provided by the borrower during registration.\n",
    "- **dti**: A ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income.\n",
    "- **emp_length**: Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years. \n",
    "- **grade**: LC assigned loan grade\n",
    "- **home_ownership**: The home ownership status provided by the borrower during registration or obtained from the credit report. Our values are: RENT, OWN, MORTGAGE, OTHER\n",
    "- **int_rate**: Interest Rate on the loan\n",
    "- **term**: The number of payments on the loan. Values are in months and can be either 36 or 60.\n",
    "- **verification_status**: Indicates if income was verified by LC, not verified, or if the income source was verified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and structure data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T20:03:33.825072Z",
     "start_time": "2020-02-13T20:03:31.779411Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>funded_amnt</th>\n",
       "      <th>funded_amnt_inv</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>installment</th>\n",
       "      <th>grade</th>\n",
       "      <th>sub_grade</th>\n",
       "      <th>emp_title</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>...</th>\n",
       "      <th>pub_rec_bankruptcies</th>\n",
       "      <th>tax_liens</th>\n",
       "      <th>hardship_flag</th>\n",
       "      <th>debt_settlement_flag</th>\n",
       "      <th>debt_settlement_flag_date</th>\n",
       "      <th>settlement_status</th>\n",
       "      <th>settlement_date</th>\n",
       "      <th>settlement_amount</th>\n",
       "      <th>settlement_percentage</th>\n",
       "      <th>settlement_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5000.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>4975.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>10.65%</td>\n",
       "      <td>162.87</td>\n",
       "      <td>B</td>\n",
       "      <td>B2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>15.27%</td>\n",
       "      <td>59.83</td>\n",
       "      <td>C</td>\n",
       "      <td>C4</td>\n",
       "      <td>Ryder</td>\n",
       "      <td>&lt; 1 year</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2400.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>15.96%</td>\n",
       "      <td>84.33</td>\n",
       "      <td>C</td>\n",
       "      <td>C5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>36 months</td>\n",
       "      <td>13.49%</td>\n",
       "      <td>339.31</td>\n",
       "      <td>C</td>\n",
       "      <td>C1</td>\n",
       "      <td>AIR RESOURCES BOARD</td>\n",
       "      <td>10+ years</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>60 months</td>\n",
       "      <td>12.69%</td>\n",
       "      <td>67.79</td>\n",
       "      <td>B</td>\n",
       "      <td>B5</td>\n",
       "      <td>University Medical Group</td>\n",
       "      <td>1 year</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   loan_amnt  funded_amnt  funded_amnt_inv        term int_rate  installment  \\\n",
       "0     5000.0       5000.0           4975.0   36 months   10.65%       162.87   \n",
       "1     2500.0       2500.0           2500.0   60 months   15.27%        59.83   \n",
       "2     2400.0       2400.0           2400.0   36 months   15.96%        84.33   \n",
       "3    10000.0      10000.0          10000.0   36 months   13.49%       339.31   \n",
       "4     3000.0       3000.0           3000.0   60 months   12.69%        67.79   \n",
       "\n",
       "  grade sub_grade                 emp_title emp_length  ...  \\\n",
       "0     B        B2                       NaN  10+ years  ...   \n",
       "1     C        C4                     Ryder   < 1 year  ...   \n",
       "2     C        C5                       NaN  10+ years  ...   \n",
       "3     C        C1       AIR RESOURCES BOARD  10+ years  ...   \n",
       "4     B        B5  University Medical Group     1 year  ...   \n",
       "\n",
       "  pub_rec_bankruptcies  tax_liens hardship_flag debt_settlement_flag  \\\n",
       "0                  0.0        0.0             N                    N   \n",
       "1                  0.0        0.0             N                    N   \n",
       "2                  0.0        0.0             N                    N   \n",
       "3                  0.0        0.0             N                    N   \n",
       "4                  0.0        0.0             N                    N   \n",
       "\n",
       "  debt_settlement_flag_date settlement_status settlement_date  \\\n",
       "0                       NaN               NaN             NaN   \n",
       "1                       NaN               NaN             NaN   \n",
       "2                       NaN               NaN             NaN   \n",
       "3                       NaN               NaN             NaN   \n",
       "4                       NaN               NaN             NaN   \n",
       "\n",
       "  settlement_amount settlement_percentage settlement_term  \n",
       "0               NaN                   NaN             NaN  \n",
       "1               NaN                   NaN             NaN  \n",
       "2               NaN                   NaN             NaN  \n",
       "3               NaN                   NaN             NaN  \n",
       "4               NaN                   NaN             NaN  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Read in csv files, store them\n",
    "dfs = [pd.read_csv(f,header=0,skiprows=1,low_memory=False) for f in filenames]\n",
    "\n",
    "# concatenate the dataframes (as standard there is only 1)\n",
    "df = pd.concat(dfs)\\\n",
    "        .dropna(subset=['loan_amnt'])\\\n",
    "        .dropna(axis=1, how='all')\n",
    "\n",
    "# View data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T20:03:39.230476Z",
     "start_time": "2020-02-13T20:03:39.019244Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>int_rate_f</th>\n",
       "      <th>emp_length_f</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>dti</th>\n",
       "      <th>charged_off</th>\n",
       "      <th>term_ 60 months</th>\n",
       "      <th>grade_B</th>\n",
       "      <th>grade_C</th>\n",
       "      <th>grade_D</th>\n",
       "      <th>grade_E</th>\n",
       "      <th>grade_F</th>\n",
       "      <th>grade_G</th>\n",
       "      <th>home_ownership_NONE</th>\n",
       "      <th>home_ownership_OTHER</th>\n",
       "      <th>home_ownership_OWN</th>\n",
       "      <th>home_ownership_RENT</th>\n",
       "      <th>verification_status_Source Verified</th>\n",
       "      <th>verification_status_Verified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.65</td>\n",
       "      <td>10.0</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>27.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.96</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12252.0</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.49</td>\n",
       "      <td>10.0</td>\n",
       "      <td>49200.0</td>\n",
       "      <td>20.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.69</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>17.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
=======
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "exercise1.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Se7enroll/SDS-econometrics/blob/master/Code/exercise1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx3sI9pwufbL",
        "colab_type": "text"
      },
      "source": [
        "> **Note:** In most sessions you will be solving exercises posed in a Jupyter notebook that looks like this one. Because you are cloning a Github repository that only we can push to, you should **NEVER EDIT** any of the files you pull from Github. Instead, what you should do, is either make a new notebook and write your solutions in there, or **make a copy of this notebook and save it somewhere else** on your computer, not inside the `sds` folder that you cloned, so you can write your answers in there. If you edit the notebook you pulled from Github, those edits (possible your solutions to the exercises) may be overwritten and lost the next time you pull from Github. This is important, so don't hesitate to ask if it is unclear."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdLYrVxHufbh",
        "colab_type": "text"
      },
      "source": [
        "# Exercise Set 1: Lending club\n",
        "\n",
        "*February 5, 2019*\n",
        "\n",
        "In this Exercise Set 1 we will investigate loan data from `Lending Club`. We will see how nested cross validation can be used  to compute a distribution of performance metrics for comparing different models. We will also try out bagging.\n",
        "\n",
        "Below we provide some code that helps by downloading and cleaning the data. Note this takes around 1 min if you have fast internet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Hv4qN7Dufbz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note: there are three .zip files with letter = a,b,c. \n",
        "# To ensure the files download in reasonable time we \n",
        "# only work with the first of the three. If you have time\n",
        "# you can modify this cell to download all three. \n",
        "\n",
        "import os\n",
        "import requests\n",
        "\n",
        "filenames = []\n",
        "base_url = 'https://resources.lendingclub.com/'\n",
        "\n",
        "letter = 'a'\n",
        "filename = f'LoanStats3{letter}.csv.zip'\n",
        "url = base_url+filename\n",
        "\n",
        "if not os.path.exists(filename):\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    open(filename, 'wb').write(r.content)\n",
        "filenames.append(filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpX6Wp2Iufch",
        "colab_type": "text"
      },
      "source": [
        "The target we are going to predict is whether loan is repaid. This can be extracted from the `loan_status` below. Features names that we work with:\n",
        "\n",
        "- **annual_inc**: The self-reported annual income provided by the borrower during registration.\n",
        "- **dti**: A ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income.\n",
        "- **emp_length**: Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years. \n",
        "- **grade**: LC assigned loan grade\n",
        "- **home_ownership**: The home ownership status provided by the borrower during registration or obtained from the credit report. Our values are: RENT, OWN, MORTGAGE, OTHER\n",
        "- **int_rate**: Interest Rate on the loan\n",
        "- **term**: The number of payments on the loan. Values are in months and can be either 36 or 60.\n",
        "- **verification_status**: Indicates if income was verified by LC, not verified, or if the income source was verified"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3uGwqksufcp",
        "colab_type": "text"
      },
      "source": [
        "Load and structure data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goXtpJD-ufc2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "outputId": "e50fb233-8e91-4d44-ea60-9e9c3f26ffeb"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Read in csv files, store them\n",
        "dfs = [pd.read_csv(f,header=0,skiprows=1,low_memory=False) for f in filenames]\n",
        "\n",
        "# concatenate the dataframes (as standard there is only 1)\n",
        "df = pd.concat(dfs)\\\n",
        "        .dropna(subset=['loan_amnt'])\\\n",
        "        .dropna(axis=1, how='all')\n",
        "\n",
        "# View data\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loan_amnt</th>\n",
              "      <th>funded_amnt</th>\n",
              "      <th>funded_amnt_inv</th>\n",
              "      <th>term</th>\n",
              "      <th>int_rate</th>\n",
              "      <th>installment</th>\n",
              "      <th>grade</th>\n",
              "      <th>sub_grade</th>\n",
              "      <th>emp_title</th>\n",
              "      <th>emp_length</th>\n",
              "      <th>home_ownership</th>\n",
              "      <th>annual_inc</th>\n",
              "      <th>verification_status</th>\n",
              "      <th>issue_d</th>\n",
              "      <th>loan_status</th>\n",
              "      <th>pymnt_plan</th>\n",
              "      <th>desc</th>\n",
              "      <th>purpose</th>\n",
              "      <th>title</th>\n",
              "      <th>zip_code</th>\n",
              "      <th>addr_state</th>\n",
              "      <th>dti</th>\n",
              "      <th>delinq_2yrs</th>\n",
              "      <th>earliest_cr_line</th>\n",
              "      <th>inq_last_6mths</th>\n",
              "      <th>mths_since_last_delinq</th>\n",
              "      <th>mths_since_last_record</th>\n",
              "      <th>open_acc</th>\n",
              "      <th>pub_rec</th>\n",
              "      <th>revol_bal</th>\n",
              "      <th>revol_util</th>\n",
              "      <th>total_acc</th>\n",
              "      <th>initial_list_status</th>\n",
              "      <th>out_prncp</th>\n",
              "      <th>out_prncp_inv</th>\n",
              "      <th>total_pymnt</th>\n",
              "      <th>total_pymnt_inv</th>\n",
              "      <th>total_rec_prncp</th>\n",
              "      <th>total_rec_int</th>\n",
              "      <th>total_rec_late_fee</th>\n",
              "      <th>recoveries</th>\n",
              "      <th>collection_recovery_fee</th>\n",
              "      <th>last_pymnt_d</th>\n",
              "      <th>last_pymnt_amnt</th>\n",
              "      <th>next_pymnt_d</th>\n",
              "      <th>last_credit_pull_d</th>\n",
              "      <th>collections_12_mths_ex_med</th>\n",
              "      <th>policy_code</th>\n",
              "      <th>application_type</th>\n",
              "      <th>acc_now_delinq</th>\n",
              "      <th>chargeoff_within_12_mths</th>\n",
              "      <th>delinq_amnt</th>\n",
              "      <th>pub_rec_bankruptcies</th>\n",
              "      <th>tax_liens</th>\n",
              "      <th>hardship_flag</th>\n",
              "      <th>debt_settlement_flag</th>\n",
              "      <th>debt_settlement_flag_date</th>\n",
              "      <th>settlement_status</th>\n",
              "      <th>settlement_date</th>\n",
              "      <th>settlement_amount</th>\n",
              "      <th>settlement_percentage</th>\n",
              "      <th>settlement_term</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5000.0</td>\n",
              "      <td>5000.0</td>\n",
              "      <td>4975.0</td>\n",
              "      <td>36 months</td>\n",
              "      <td>10.65%</td>\n",
              "      <td>162.87</td>\n",
              "      <td>B</td>\n",
              "      <td>B2</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10+ years</td>\n",
              "      <td>RENT</td>\n",
              "      <td>24000.0</td>\n",
              "      <td>Verified</td>\n",
              "      <td>Dec-2011</td>\n",
              "      <td>Fully Paid</td>\n",
              "      <td>n</td>\n",
              "      <td>Borrower added on 12/22/11 &gt; I need to upgra...</td>\n",
              "      <td>credit_card</td>\n",
              "      <td>Computer</td>\n",
              "      <td>860xx</td>\n",
              "      <td>AZ</td>\n",
              "      <td>27.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Jan-1985</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13648.0</td>\n",
              "      <td>83.7%</td>\n",
              "      <td>9.0</td>\n",
              "      <td>f</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5863.155187</td>\n",
              "      <td>5833.84</td>\n",
              "      <td>5000.00</td>\n",
              "      <td>863.16</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>Jan-2015</td>\n",
              "      <td>171.62</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Jan-2020</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Individual</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>2500.0</td>\n",
              "      <td>60 months</td>\n",
              "      <td>15.27%</td>\n",
              "      <td>59.83</td>\n",
              "      <td>C</td>\n",
              "      <td>C4</td>\n",
              "      <td>Ryder</td>\n",
              "      <td>&lt; 1 year</td>\n",
              "      <td>RENT</td>\n",
              "      <td>30000.0</td>\n",
              "      <td>Source Verified</td>\n",
              "      <td>Dec-2011</td>\n",
              "      <td>Charged Off</td>\n",
              "      <td>n</td>\n",
              "      <td>Borrower added on 12/22/11 &gt; I plan to use t...</td>\n",
              "      <td>car</td>\n",
              "      <td>bike</td>\n",
              "      <td>309xx</td>\n",
              "      <td>GA</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Apr-1999</td>\n",
              "      <td>5.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1687.0</td>\n",
              "      <td>9.4%</td>\n",
              "      <td>4.0</td>\n",
              "      <td>f</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1014.530000</td>\n",
              "      <td>1014.53</td>\n",
              "      <td>456.46</td>\n",
              "      <td>435.17</td>\n",
              "      <td>0.00</td>\n",
              "      <td>122.9</td>\n",
              "      <td>1.11</td>\n",
              "      <td>Apr-2013</td>\n",
              "      <td>119.66</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Oct-2016</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Individual</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2400.0</td>\n",
              "      <td>2400.0</td>\n",
              "      <td>2400.0</td>\n",
              "      <td>36 months</td>\n",
              "      <td>15.96%</td>\n",
              "      <td>84.33</td>\n",
              "      <td>C</td>\n",
              "      <td>C5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10+ years</td>\n",
              "      <td>RENT</td>\n",
              "      <td>12252.0</td>\n",
              "      <td>Not Verified</td>\n",
              "      <td>Dec-2011</td>\n",
              "      <td>Fully Paid</td>\n",
              "      <td>n</td>\n",
              "      <td>NaN</td>\n",
              "      <td>small_business</td>\n",
              "      <td>real estate business</td>\n",
              "      <td>606xx</td>\n",
              "      <td>IL</td>\n",
              "      <td>8.72</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Nov-2001</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2956.0</td>\n",
              "      <td>98.5%</td>\n",
              "      <td>10.0</td>\n",
              "      <td>f</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3005.666844</td>\n",
              "      <td>3005.67</td>\n",
              "      <td>2400.00</td>\n",
              "      <td>605.67</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>Jun-2014</td>\n",
              "      <td>649.91</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Jun-2017</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Individual</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>10000.0</td>\n",
              "      <td>10000.0</td>\n",
              "      <td>10000.0</td>\n",
              "      <td>36 months</td>\n",
              "      <td>13.49%</td>\n",
              "      <td>339.31</td>\n",
              "      <td>C</td>\n",
              "      <td>C1</td>\n",
              "      <td>AIR RESOURCES BOARD</td>\n",
              "      <td>10+ years</td>\n",
              "      <td>RENT</td>\n",
              "      <td>49200.0</td>\n",
              "      <td>Source Verified</td>\n",
              "      <td>Dec-2011</td>\n",
              "      <td>Fully Paid</td>\n",
              "      <td>n</td>\n",
              "      <td>Borrower added on 12/21/11 &gt; to pay for prop...</td>\n",
              "      <td>other</td>\n",
              "      <td>personel</td>\n",
              "      <td>917xx</td>\n",
              "      <td>CA</td>\n",
              "      <td>20.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Feb-1996</td>\n",
              "      <td>1.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5598.0</td>\n",
              "      <td>21%</td>\n",
              "      <td>37.0</td>\n",
              "      <td>f</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12231.890000</td>\n",
              "      <td>12231.89</td>\n",
              "      <td>10000.00</td>\n",
              "      <td>2214.92</td>\n",
              "      <td>16.97</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>Jan-2015</td>\n",
              "      <td>357.48</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Apr-2016</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Individual</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3000.0</td>\n",
              "      <td>3000.0</td>\n",
              "      <td>3000.0</td>\n",
              "      <td>60 months</td>\n",
              "      <td>12.69%</td>\n",
              "      <td>67.79</td>\n",
              "      <td>B</td>\n",
              "      <td>B5</td>\n",
              "      <td>University Medical Group</td>\n",
              "      <td>1 year</td>\n",
              "      <td>RENT</td>\n",
              "      <td>80000.0</td>\n",
              "      <td>Source Verified</td>\n",
              "      <td>Dec-2011</td>\n",
              "      <td>Fully Paid</td>\n",
              "      <td>n</td>\n",
              "      <td>Borrower added on 12/21/11 &gt; I plan on combi...</td>\n",
              "      <td>other</td>\n",
              "      <td>Personal</td>\n",
              "      <td>972xx</td>\n",
              "      <td>OR</td>\n",
              "      <td>17.94</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Jan-1996</td>\n",
              "      <td>0.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>27783.0</td>\n",
              "      <td>53.9%</td>\n",
              "      <td>38.0</td>\n",
              "      <td>f</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4066.908161</td>\n",
              "      <td>4066.91</td>\n",
              "      <td>3000.00</td>\n",
              "      <td>1066.91</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>Jan-2017</td>\n",
              "      <td>67.30</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Apr-2018</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Individual</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>N</td>\n",
              "      <td>N</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   loan_amnt  funded_amnt  ...  settlement_percentage settlement_term\n",
              "0     5000.0       5000.0  ...                    NaN             NaN\n",
              "1     2500.0       2500.0  ...                    NaN             NaN\n",
              "2     2400.0       2400.0  ...                    NaN             NaN\n",
              "3    10000.0      10000.0  ...                    NaN             NaN\n",
              "4     3000.0       3000.0  ...                    NaN             NaN\n",
              "\n",
              "[5 rows x 62 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hBKPccbufdY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "8cb7ba81-9e4f-4783-91dd-cb90d9f8f3a2"
      },
      "source": [
        "# Identify loans of interest\n",
        "df = df.loc[df.loan_status.isin(['Fully Paid', 'Charged Off'])].copy()\n",
        "\n",
        "# Clean up variables \n",
        "df['charged_off'] = (df.loan_status=='Charged Off').astype(int)\n",
        "df['int_rate_f'] = df.int_rate.str[:-1].astype(float)\n",
        "df['emp_length_f'] = df.emp_length\\\n",
        "                        .str.split(' ')\\\n",
        "                        .str[0].str[:2]\\\n",
        "                        .str.replace('<','0')\\\n",
        "                        .astype(float)\n",
        "\n",
        "# label and features\n",
        "y_var = 'charged_off'\n",
        "X_vars = ['term', 'int_rate_f', 'grade', 'home_ownership', 'emp_length_f',\n",
        "          'annual_inc', 'verification_status', 'dti']\n",
        "\n",
        "# Create dummies\n",
        "data = pd.get_dummies(df[X_vars+[y_var]], drop_first=True)\\\n",
        "        .dropna()\\\n",
        "        .reset_index(drop=True)\\\n",
        "        .astype(np.float64)\\\n",
        "        .loc[:2000]\\\n",
        "        .copy()\n",
        "\n",
        "# View data\n",
        "data.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>int_rate_f</th>\n",
              "      <th>emp_length_f</th>\n",
              "      <th>annual_inc</th>\n",
              "      <th>dti</th>\n",
              "      <th>charged_off</th>\n",
              "      <th>term_ 60 months</th>\n",
              "      <th>grade_B</th>\n",
              "      <th>grade_C</th>\n",
              "      <th>grade_D</th>\n",
              "      <th>grade_E</th>\n",
              "      <th>grade_F</th>\n",
              "      <th>grade_G</th>\n",
              "      <th>home_ownership_NONE</th>\n",
              "      <th>home_ownership_OTHER</th>\n",
              "      <th>home_ownership_OWN</th>\n",
              "      <th>home_ownership_RENT</th>\n",
              "      <th>verification_status_Source Verified</th>\n",
              "      <th>verification_status_Verified</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>10.65</td>\n",
              "      <td>10.0</td>\n",
              "      <td>24000.0</td>\n",
              "      <td>27.65</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15.27</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30000.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>15.96</td>\n",
              "      <td>10.0</td>\n",
              "      <td>12252.0</td>\n",
              "      <td>8.72</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13.49</td>\n",
              "      <td>10.0</td>\n",
              "      <td>49200.0</td>\n",
              "      <td>20.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12.69</td>\n",
              "      <td>1.0</td>\n",
              "      <td>80000.0</td>\n",
              "      <td>17.94</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   int_rate_f  ...  verification_status_Verified\n",
              "0       10.65  ...                           1.0\n",
              "1       15.27  ...                           0.0\n",
              "2       15.96  ...                           0.0\n",
              "3       13.49  ...                           0.0\n",
              "4       12.69  ...                           0.0\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWyPyHttufeB",
        "colab_type": "text"
      },
      "source": [
        "Now let us split the data into test and training data. To do this we use the `StratifiedShuffleSplit` from scikit learn (read more about splitting methods [here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FsrpKB2YufeM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "sss = StratifiedShuffleSplit(n_splits=10, test_size=.3, random_state=3)\n",
        "\n",
        "# These are the row indices of the stratified split\n",
        "data_splits = list(sss.split(data[y_var], data[y_var]))\n",
        "\n",
        "# Separate data in y,X\n",
        "y = data[y_var]\n",
        "X_vars_b = data.columns!=y_var\n",
        "X = data.loc[:,X_vars_b]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1YqdMsnufeu",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "\n",
        "## 1.1 Model validation\n",
        "\n",
        "We start out with some basic concepts and our goal is to estimate and evaluate the logistic regression.\n",
        "\n",
        "\n",
        "> **Ex. 1.1.1:** extract the first data split and construct features, target for train, test data.\n",
        ">> *Hint:* `data_splits` is a list with 10 train-test pairs of randomly selected row indices. So the k'th element of `data_splits` contains a pair (train_idx, test_idx) identifying the k'th fold in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAIJN0Qnufe2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# [Answer to ex. 1.1.1 here]\n",
        "i = 0\n",
        "train_index, test_index = data_splits[i]\n",
        "X_train = X.iloc[train_index, :]\n",
        "X_test = X.iloc[test_index, :]\n",
        "y_train = y.iloc[train_index]\n",
        "y_test = y.iloc[test_index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d-pIPIxuffN",
        "colab_type": "text"
      },
      "source": [
        "> **Ex. 1.1.2:** estimate a logistic regression model on your training data. Then compute the overall accuracy and $F_1$ score for the default class *on the training data*.\n",
        ">\n",
        ">> **Note**: the code below implements a pipeline which standardizes the data by converting it into zero mean and unit standard deviation. We let `C=10**10` which corresponds to no regularization. Make sure you understand why each of these steps are important by looking up the scikit learn docs.\n",
        ">\n",
        ">> *Hint 2:* `sklearn` has these functions built-in and are named `f1_score`, `accuracy_score`. See Raschka pp. 189-193."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaeCUDdLuffU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler        # scales variables to be mean=0,sd=1\n",
        "from sklearn.linear_model import LogisticRegression     # regression model\n",
        "from sklearn.pipeline import Pipeline                   # For building our model pipeline\n",
        "\n",
        "lr = Pipeline([('scale', StandardScaler()),\n",
        "               ('clf', LogisticRegression(class_weight='balanced',C=10**10, solver = 'liblinear'))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAkAcrlSuffw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c2aba5cb-a2f1-444b-cc39-333c2d0ec74e"
      },
      "source": [
        "# [Answer to ex. 1.1.2 here]\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "lr.fit(X_train, y_train)\n",
        "y_pred = lr.predict(X_train)\n",
        "f1 = f1_score(y_pred, y_train)\n",
        "acc = accuracy_score(y_pred, y_train)\n",
        "print(f'The f1 score was {f1} and the accuracy was {acc}')\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The f1 score was 0.4025316455696203 and the accuracy was 0.6628571428571428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fShY2kMfufgI",
        "colab_type": "text"
      },
      "source": [
        "> **Ex. 1.1.3:** Explain some advantages of computing the $F_1$ score vs. computing the overall accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTm0JEp0ufgQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# [Answer to ex. 1.1.3 here]\n",
        "# Simplisticly Accuray meassures number of true positives an true negatives, whereas f1 scores lay most weight to false positives and false negativs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCmzzXMcufgz",
        "colab_type": "text"
      },
      "source": [
        "> **Ex. 1.1.4:** Compute the *test set* accuracy and $F_1$ score. How does these compare to the results you got in exercise 1.1.3? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcAXLnjoufg9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "f5c8533d-883b-4160-cf6e-b03591a4c185"
      },
      "source": [
        "# [Answer to ex. 1.1.4 here]\n",
        "y_pred_test = lr.predict(X_test)\n",
        "f1_test = f1_score(y_pred_test, y_test)\n",
        "acc_test = accuracy_score(y_pred_test, y_test)\n",
        "print(f'For the test data the f1 score was {f1_test} and the accuracy was {acc_test}')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For the test data the f1 score was 0.33727810650887574 and the accuracy was 0.627287853577371\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kwtnKIUufhZ",
        "colab_type": "text"
      },
      "source": [
        "> **Ex. 1.1.5:** Explain why test set performance is often preferred to the training set performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLZdYArFufhi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# [Answer to ex. 1.1.5 here]\n",
        "# Because the test data is \"unseen\" and therefore much less prone to overfitting"
>>>>>>> Created using Colaboratory
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqza7VNhufh8",
        "colab_type": "text"
      },
      "source": [
        "<br>\n",
        "\n",
        "### Cross validation\n",
        "\n",
        "We now turn to actually optimizing the model. We will use cross validation to perform the optimization.\n",
        "\n",
        "> **Ex. 1.1.6:** Explain what the parameter `C` does in logistic regression. How do parameters change when `C` get smaller? Also explain what `penalty='l1'` will do for coefficients.\n",
        ">> *Hint:* The documentation for scikit learn will tell you everything you need to know about their implementation of logistic regression."
      ]
<<<<<<< HEAD
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify loans of interest\n",
    "df = df.loc[df.loan_status.isin(['Fully Paid', 'Charged Off'])].copy()\n",
    "\n",
    "# Clean up variables \n",
    "df['charged_off'] = (df.loan_status=='Charged Off').astype(int)\n",
    "df['int_rate_f'] = df.int_rate.str[:-1].astype(float)\n",
    "df['emp_length_f'] = df.emp_length\\\n",
    "                        .str.split(' ')\\\n",
    "                        .str[0].str[:2]\\\n",
    "                        .str.replace('<','0')\\\n",
    "                        .astype(float)\n",
    "\n",
    "# label and features\n",
    "y_var = 'charged_off'\n",
    "X_vars = ['term', 'int_rate_f', 'grade', 'home_ownership', 'emp_length_f',\n",
    "          'annual_inc', 'verification_status', 'dti']\n",
    "\n",
    "# Create dummies\n",
    "data = pd.get_dummies(df[X_vars+[y_var]], drop_first=True)\\\n",
    "        .dropna()\\\n",
    "        .reset_index(drop=True)\\\n",
    "        .astype(np.float64)\\\n",
    "        .loc[:2000]\\\n",
    "        .copy()\n",
    "\n",
    "# View data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us split the data into test and training data. To do this we use the `StratifiedShuffleSplit` from scikit learn (read more about splitting methods [here](https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-13T20:03:44.169960Z",
     "start_time": "2020-02-13T20:03:43.120681Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=10, test_size=.3, random_state=3)\n",
    "\n",
    "# These are the row indices of the stratified split\n",
    "data_splits = list(sss.split(data[y_var], data[y_var]))\n",
    "\n",
    "# Separate data in y,X\n",
    "y = data[y_var]\n",
    "X_vars_b = data.columns!=y_var\n",
    "X = data.loc[:,X_vars_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T08:38:36.986114Z",
     "start_time": "2020-02-14T08:38:36.983301Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T08:38:37.567425Z",
     "start_time": "2020-02-14T08:38:37.517455Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14870951, 0.14312772, 0.24864011, 0.28116442, 0.02859813,\n",
       "       0.0113827 , 0.01034434, 0.01017024, 0.00895967, 0.01040957,\n",
       "       0.00161948, 0.        , 0.        , 0.0140203 , 0.0299059 ,\n",
       "       0.02835375, 0.02459417])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)\n",
    "model.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "## 1.1 Model validation\n",
    "\n",
    "We start out with some basic concepts and our goal is to estimate and evaluate the logistic regression.\n",
    "\n",
    "\n",
    "> **Ex. 1.1.1:** extract the first data split and construct features, target for train, test data.\n",
    ">> *Hint:* `data_splits` is a list with 10 train-test pairs of randomly selected row indices. So the k'th element of `data_splits` contains a pair (train_idx, test_idx) identifying the k'th fold in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 1.1.1 here]\n",
    "# extract first set of indexes\n",
    "first_is = data_splits[0]\n",
    "train_is, test_is = first_is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract data\n",
    "train, test = data.iloc[train_is,:], data.iloc[test_is,:]\n",
    "y_train, y_test = train[y_var], test[y_var]\n",
    "X_train, X_test = train.loc[:,X_vars_b], test.loc[:,X_vars_b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 1.1.2:** estimate a logistic regression model on your training data. Then compute the overall accuracy and $F_1$ score for the default class *on the training data*.\n",
    ">\n",
    ">> **Note**: the code below implements a pipeline which standardizes the data by converting it into zero mean and unit standard deviation. We let `C=10**10` which corresponds to no regularization. Make sure you understand why each of these steps are important by looking up the scikit learn docs.\n",
    ">\n",
    ">> *Hint 2:* `sklearn` has these functions built-in and are named `f1_score`, `accuracy_score`. See Raschka pp. 189-193."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler        # scales variables to be mean=0,sd=1\n",
    "from sklearn.linear_model import LogisticRegression     # regression model\n",
    "from sklearn.pipeline import Pipeline                   # For building our model pipeline\n",
    "\n",
    "lr = Pipeline([('scale', StandardScaler()),\n",
    "               ('clf', LogisticRegression(class_weight='balanced',C=10**10, solver = 'liblinear'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 1.1.2 here]\n",
    "model = lr.fit(X_train, y_train)\n",
    "pred_train = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc was 0.6592857142857143 and f1 was 0.39390088945362134\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6592857142857143, 0.39390088945362134)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "def gibScores(true, pred):\n",
    "    acc = accuracy_score(true, pred)\n",
    "    f1 = f1_score(true, pred)\n",
    "    print(f\"Acc was {acc} and f1 was {f1}\")\n",
    "    return acc, f1\n",
    "gibScores(y_train, pred_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 1.1.3:** Explain some advantages of computing the $F_1$ score vs. computing the overall accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is better when the True Positives and True negatives are important while F1-score is better when the False Negatives and False Positives are important. Accuracy can be utilized when the class dissemination is comparable while F1-score is a superior metric when there are imbalanced classes as in the above case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 1.1.4:** Compute the *test set* accuracy and $F_1$ score. How does these compare to the results you got in exercise 1.1.3? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc was 0.6206322795341098 and f1 was 0.33720930232558144\n"
     ]
    }
   ],
   "source": [
    "# [Answer to ex. 1.1.4 here]\n",
    "pred_test = model.predict(X_test)\n",
    "gibScores(y_test, pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 1.1.5:** Explain why test set performance is often preferred to the training set performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test data should be out of sample, and are thus not prone to overfitting and will test whether the model has \"fitted the points/observations or the general structure\" - the latter being the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Cross validation\n",
    "\n",
    "We now turn to actually optimizing the model. We will use cross validation to perform the optimization.\n",
    "\n",
    "> **Ex. 1.1.6:** Explain what the parameter `C` does in logistic regression. How do parameters change when `C` get smaller? Also explain what `penalty='l1'` will do for coefficients.\n",
    ">> *Hint:* The documentation for scikit learn will tell you everything you need to know about their implementation of logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C is the inverse regularization weight when c aproaches zero, the reularization tends to infinity. L1 is regularization is also known as the Lasso nornmalization, which adds a absolute value term to the objective function as a penalty for large values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 1.1.7:** Use cross validation on training set to estimate hyperparameters and then re-estimate model on training set. The set of $\\lambda$ to be considered are $\\{10^{-4},10^{-2},1,10^{2},10^{4}\\}$. Is the model with optimized hyperparameters better than the non-optimized?\n",
    ">\n",
    ">> *Hint 1:* This procedure is implemented in `GridSearchCV`. You set `n_jobs=-1` to parallize computation. See Raschka pp. 186-187 for inspiration.\n",
    ">\n",
    ">> *Hint 2:* Consider using `np.logspace` for making the set of $\\lambda$.m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    1.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('scale',\n",
       "                                        StandardScaler(copy=True,\n",
       "                                                       with_mean=True,\n",
       "                                                       with_std=True)),\n",
       "                                       ('clf',\n",
       "                                        LogisticRegression(C=10000000000,\n",
       "                                                           class_weight='balanced',\n",
       "                                                           dual=False,\n",
       "                                                           fit_intercept=True,\n",
       "                                                           intercept_scaling=1,\n",
       "                                                           l1_ratio=None,\n",
       "                                                           max_iter=100,\n",
       "                                                           multi_class='auto',\n",
       "                                                           n_jobs=None,\n",
       "                                                           penalty='l2',\n",
       "                                                           random_state=None,\n",
       "                                                           solver='l...\n",
       "                                                    intercept_scaling=1,\n",
       "                                                    l1_ratio=None, max_iter=100,\n",
       "                                                    multi_class='auto',\n",
       "                                                    n_jobs=None, penalty='l1',\n",
       "                                                    random_state=None,\n",
       "                                                    solver='liblinear',\n",
       "                                                    tol=0.0001, verbose=0,\n",
       "                                                    warm_start=False)],\n",
       "                         'clf__C': array([1.e-04, 1.e-02, 1.e+00, 1.e+02, 1.e+04]),\n",
       "                         'clf__penalty': ['l1'], 'clf__solver': ['liblinear']},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=True)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [Answer to ex. 1.1.7 here]\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params={\"clf\": [LogisticRegression()],\n",
    "        \"clf__penalty\": ['l1'],\n",
    "        \"clf__solver\": ['liblinear'],\n",
    "        \"clf__C\":np.logspace(-4,4,num=5), # lamdas in 10e-4-10e4\n",
    "}\n",
    "# use LR defined earlier, the splits in cv is using the already shuffled data\n",
    "GCV=GridSearchCV(lr,param_grid=params, cv=10, n_jobs=-1, verbose=True)\n",
    "GCV.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "results: {'clf': LogisticRegression(C=100.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
      "                   multi_class='auto', n_jobs=None, penalty='l1',\n",
      "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
      "                   warm_start=False), 'clf__C': 100.0, 'clf__penalty': 'l1', 'clf__solver': 'liblinear'}\n",
      "Acc was 0.8202995008319468 and f1 was 0.06896551724137931\n"
     ]
    }
   ],
   "source": [
    "print(\"results:\", GCV.best_params_)\n",
    "gibScores(GCV.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 1.1.8:** Apply nested resampling to compute a distribution of test scores with and without optimization. You should use `data_splits` which we defined initially and input all the data.\n",
    ">\n",
    ">> *Hint:* You can implement this using your code from Ex. 1.1.6 and combine it with `cross_val_score`. Note that `cv` input should use `data_splits`. See Raschka pp. 188-189 for inspiration. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
      "Fitting 10 folds for each of 5 candidates, totalling 50 fits\n",
      "[0.8202995  0.82196339 0.82196339 0.82695507 0.82196339 0.82196339\n",
      " 0.82196339 0.82529118 0.82196339 0.82196339]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "# [Answer to ex. 1.1.8 here]\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# Reuse non-nested from 1.1.7\n",
    "non_nested_score = GCV.best_score_\n",
    "\n",
    "# Nested CV with parameter optimization\n",
    "nested_score = cross_val_score(GCV, X=X, y=y, cv=data_splits)\n",
    "print(nested_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00327193  0.00160803  0.00160803 -0.00338365  0.00160803  0.00160803\n",
      "  0.00160803 -0.00171975  0.00160803  0.00160803]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.8202995 , 0.82196339, 0.82196339, 0.82695507, 0.82196339,\n",
       "       0.82196339, 0.82196339, 0.82529118, 0.82196339, 0.82196339])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(non_nested_score - nested_score)\n",
    "nested_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc was 0.6206322795341098 and f1 was 0.33720930232558144\n",
      "Acc was 0.6572379367720466 and f1 was 0.4011627906976744\n",
      "Acc was 0.6555740432612313 and f1 was 0.3076923076923077\n",
      "Acc was 0.610648918469218 and f1 was 0.360655737704918\n",
      "Acc was 0.6139767054908486 and f1 was 0.35195530726256985\n",
      "Acc was 0.6439267886855241 and f1 was 0.3395061728395062\n",
      "Acc was 0.6023294509151415 and f1 was 0.33795013850415506\n",
      "Acc was 0.6455906821963394 and f1 was 0.3716814159292035\n",
      "Acc was 0.6306156405990017 and f1 was 0.3901098901098901\n",
      "Acc was 0.6589018302828619 and f1 was 0.3730886850152905\n"
     ]
    }
   ],
   "source": [
    "accs = []\n",
    "f1s = []\n",
    "for k, (train, test) in enumerate(data_splits):\n",
    "    lr.fit(X.iloc[train], y.iloc[train])\n",
    "    pred = lr.predict(X.iloc[test])\n",
    "    acc, f1 = gibScores(y.iloc[test], pred)\n",
    "    accs.append(acc)\n",
    "    f1s.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(lr, X=X, y=y, cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64676617, 0.665     , 0.665     , 0.565     , 0.615     ,\n",
       "       0.61      , 0.635     , 0.685     , 0.635     , 0.655     ])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 1.1.9:** Plot the distributions of the optimized vs. the non-optimized. What do you conclude about the performance difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 1.1.9 here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  **Ex. 1.1.10** Argue how $F_1$ differs from  *area under the curve* (AUC). Comment on their theoretical properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification trees and forests\n",
    "\n",
    ">  **Ex. 1.1.11** Estimate a classification tree on the training data (with default hyperparameters). Evalate both on training and test data by computing the *area under the curve*.\n",
    ">\n",
    ">> *Hint:* You can check out code for Ex. 1.1.10 for inspiration. You may also want to look up `roc_auc_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 1.1.11 here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  **Ex. 1.1.12** Estimate 50 classification trees and compute the mean prediction. Use the mean prediction to compute the *area under the curve*.\n",
    ">\n",
    ">> *Hint:* You may use the code block below and apply it repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def train_tree(data_train):\n",
    "    '''\n",
    "    Trains a decision tree on a bootstrap \n",
    "    of training data.\n",
    "    '''\n",
    "    df_ = data_train.sample(frac=1, replace=True)\n",
    "    ct = DecisionTreeClassifier(class_weight='balanced')\n",
    "    ct.fit(df_.loc[:,X_vars_b], df_[y_var])\n",
    "    return ct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 1.1.12 here]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">  **Ex. 1.1.13** Is Random Forest classification different from the procedure of aggregating tree predictions above? If so, explain how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Answer to ex. 1.1.13 here]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
=======
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRNsAY9dufiH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# [Answer to ex. 1.1.6 here]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuMMONrgufik",
        "colab_type": "text"
      },
      "source": [
        "> **Ex. 1.1.7:** Use cross validation on training set to estimate hyperparameters and then re-estimate model on training set. The set of $\\lambda$ to be considered are $\\{10^{-4},10^{-2},1,10^{2},10^{4}\\}$. Is the model with optimized hyperparameters better than the non-optimized?\n",
        ">\n",
        ">> *Hint 1:* This procedure is implemented in `GridSearchCV`. You set `n_jobs=-1` to parallize computation. See Raschka pp. 186-187 for inspiration.\n",
        ">\n",
        ">> *Hint 2:* Consider using `np.logspace` for making the set of $\\lambda$.m"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0m_34_0Wufiq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# [Answer to ex. 1.1.7 here]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxTgn_6Oufi_",
        "colab_type": "text"
      },
      "source": [
        "> **Ex. 1.1.8:** Apply nested resampling to compute a distribution of test scores with and without optimization. You should use `data_splits` which we defined initially and input all the data.\n",
        ">\n",
        ">> *Hint:* You can implement this using your code from Ex. 1.1.6 and combine it with `cross_val_score`. Note that `cv` input should use `data_splits`. See Raschka pp. 188-189 for inspiration. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feyQP73DufjE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# [Answer to ex. 1.1.8 here]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cmiqt5rcufjp",
        "colab_type": "text"
      },
      "source": [
        "> **Ex. 1.1.9:** Plot the distributions of the optimized vs. the non-optimized. What do you conclude about the performance difference?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUjZrRetufjy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# [Answer to ex. 1.1.9 here]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRxkadrJufkF",
        "colab_type": "text"
      },
      "source": [
        ">  **Ex. 1.1.10** Argue how $F_1$ differs from  *area under the curve* (AUC). Comment on their theoretical properties."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f51124sKufkN",
        "colab_type": "text"
      },
      "source": [
        "### Classification trees and forests\n",
        "\n",
        ">  **Ex. 1.1.11** Estimate a classification tree on the training data (with default hyperparameters). Evalate both on training and test data by computing the *area under the curve*.\n",
        ">\n",
        ">> *Hint:* You can check out code for Ex. 1.1.10 for inspiration. You may also want to look up `roc_auc_score`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjRTpp-NufkT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# [Answer to ex. 1.1.11 here]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynPTwJ1yuflJ",
        "colab_type": "text"
      },
      "source": [
        ">  **Ex. 1.1.12** Estimate 50 classification trees and compute the mean prediction. Use the mean prediction to compute the *area under the curve*.\n",
        ">\n",
        ">> *Hint:* You may use the code block below and apply it repeatedly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnGscprkuflN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "def train_tree(data_train):\n",
        "    '''\n",
        "    Trains a decision tree on a bootstrap \n",
        "    of training data.\n",
        "    '''\n",
        "    df_ = data_train.sample(frac=1, replace=True)\n",
        "    ct = DecisionTreeClassifier(class_weight='balanced')\n",
        "    ct.fit(df_.loc[:,X_vars_b], df_[y_var])\n",
        "    return ct\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwQUWFQXufli",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# [Answer to ex. 1.1.12 here]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyQlIVKwuflv",
        "colab_type": "text"
      },
      "source": [
        ">  **Ex. 1.1.13** Is Random Forest classification different from the procedure of aggregating tree predictions above? If so, explain how."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qsdcNX5Suflz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# [Answer to ex. 1.1.13 here]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
>>>>>>> Created using Colaboratory
